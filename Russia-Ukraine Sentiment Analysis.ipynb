{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import yaml\n",
    "import tweepy\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import networkx\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting tweets\n",
    "We use tweepy which is built on Twitter API to collect tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as file:\n",
    "    keys = yaml.safe_load(file)\n",
    "    consumer_key = keys[\"search_tweets_api\"][\"consumer_key\"]\n",
    "    consumer_secret = keys[\"search_tweets_api\"][\"consumer_secret\"]\n",
    "    access_token = keys[\"search_tweets_api\"][\"access_token\"]\n",
    "    access_token_secret = keys[\"search_tweets_api\"][\"access_token_secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    try:\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    except:\n",
    "        print(\"An error occurred during the authentication\")\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove URL\n",
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that Twitter’s search service is not meant to be an exhaustive source of Tweets. Not all Tweets will be indexed or made available via the search interface. Therefore, we focus on random sampling on users' sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_hashtag(api, date_until, words):\n",
    "    df = pd.DataFrame(columns=['id', 'created_at', 'username', 'location', 'following', \n",
    "                               'followers', 'retweetcount', 'text']) \n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=words, lang=\"en\", until=date_until, tweet_mode='extended').items() \n",
    "    list_tweets = [tweet for tweet in tweets] \n",
    "         \n",
    "    for tweet in list_tweets: \n",
    "        id = tweet.id\n",
    "        created_at = tweet.created_at\n",
    "        username = tweet.user.screen_name \n",
    "        location = tweet.user.location \n",
    "        following = tweet.user.friends_count \n",
    "        followers = tweet.user.followers_count \n",
    "        totaltweets = tweet.user.statuses_count \n",
    "        retweetcount = tweet.retweet_count \n",
    "\n",
    "        try: \n",
    "            text = tweet.retweeted_status.full_text \n",
    "        except AttributeError: \n",
    "            text = tweet.full_text \n",
    "        text = remove_url(text)\n",
    "  \n",
    "        tweets = [id, created_at, username, location, following, \n",
    "                     followers, retweetcount, text] \n",
    "\n",
    "        df.loc[len(df)] = tweets \n",
    "          \n",
    "    filename = 'tweets.csv'\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 3\n",
      "Rate limit reached. Sleeping for: 851\n",
      "Rate limit reached. Sleeping for: 851\n",
      "Rate limit reached. Sleeping for: 850\n",
      "Rate limit reached. Sleeping for: 853\n"
     ]
    }
   ],
   "source": [
    "api = auth()\n",
    "words = \"#UkraineRussia\"\n",
    "date_until = \"2022-02-24\"\n",
    "search_by_hashtag(api, date_until, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "## Microsoft Azure’s Text Analytics\n",
    "We first adopt Cognitive Service to obtain the sentiment score of tweets.\n",
    "The score outputted is in range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_azure(data):\n",
    "    azure_url = \"https://urwarsentiment.cognitiveservices.azure.com/\"\n",
    "    language_api_url = \"{}text/analytics/v2.1/languages\".format(azure_url)\n",
    "    sentiment_url = \"{}text/analytics/v2.1/sentiment\".format(azure_url)\n",
    "    subscription_key = data[\"azure\"][\"subscription_key\"]\n",
    "    return language_api_url, sentiment_url, subscription_key\n",
    "\n",
    "def azure_header(subscription_key):\n",
    "    return {\"Ocp-Apim-Subscription-Key\": subscription_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to further format the data collected. The final payload to Azure endpoint should contain text and language only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_lang_data(df):\n",
    "    text_lan = df[\"text\"]\n",
    "    text_lan[\"language\"] = \"en\"\n",
    "    json_lines = text_lan.to_json(orient=\"records\")\n",
    "    return json_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_format(json_lines):\n",
    "    docu_format = '\"' + \"documents\" + '\"'\n",
    "    json_docu_format = \"{}:{}\".format(docu_format, json_lines)\n",
    "    docu_align = \"{\" + json_docu_format + \"}\"\n",
    "    jd_align = json.dumps(docu_align)\n",
    "    jl_align = json.loads(jd_align)\n",
    "    return ast.literal_eval(jl_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(headers, sentiment_url, document_format):\n",
    "    response = requests.post(\n",
    "        sentiment_url, headers=headers, json=document_format)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = azure_header(subscription_key)\n",
    "json_lines = combine_lang_data(documents, with_languages)\n",
    "document_format = add_document_format(json_lines)\n",
    "sentiments = sentiment_scores(headers, sentiment_url, document_format)\n",
    "df[\"azure_polar\"] = sentiments[\"documents\"][\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "Textblob is an open-source python library for processing textual data. It can evaluate both polarity and subjectivity in text. The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_objects = [TextBlob(tweet) for tweet in list(df['text'])]\n",
    "blob_polar = [tweet.sentiment.polarity for tweet in sentiment_objects]\n",
    "blob_subj = [tweet.sentiment.subj for tweet in sentiment_objects]\n",
    "df[\"blob_polar\"] = blob_polar\n",
    "df[\"blob_subj\"] = blob_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1, ax2 = plt.subplots(2, 1, 1)\n",
    "df.hist(column=\"blob_polar\", bins=[-1, -0.75, -0.5, -0.25, 0.25, 0.5, 0.75, 1], ax=ax1)\n",
    "ax1.set_title(\"Sentiments from Tweets on Ukraine-Russia War\")\n",
    "df.hist(column=\"blob_subj\", bins=[0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1], ax=ax2)\n",
    "ax1.set_title(\"Subjectivity from Tweets on Ukraine-Russia War\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4486cc46800a3a36dbbddfd8b12a33d5293b2126d0bd638d95bed74d24a9d28"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
